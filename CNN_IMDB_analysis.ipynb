{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d5134282",
      "metadata": {
        "id": "d5134282"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple\n",
        "\n",
        "from tensorflow.keras import (\n",
        "    layers, \n",
        "    losses, \n",
        "    utils,\n",
        ")\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "197b6dea",
      "metadata": {
        "id": "197b6dea"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "c293beb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c293beb3",
        "outputId": "79c11678-f1a5-4965-a10c-7d789b3f0450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(tf.config.list_logical_devices(\"CPU\"))\n",
        "print(tf.config.list_physical_devices(\"GPU\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7522991",
      "metadata": {
        "id": "d7522991"
      },
      "source": [
        "#TODO: use imdb.vocab and read imdb error and readme \n",
        "### Dataset review\n",
        "\n",
        "Given [dataset](http://ai.stanford.edu/~amaas/data/sentiment/) consists of 50,000 reviews split evenly into 25k train and 25k test sets. Generally, there are redudant amount of data for testing.  \n",
        "- So it makes sense to split data to 80/20 or 90/10.\n",
        "\n",
        "The next one is that, bag of words `imdb.vocab`  and `already-tokenized bag of words (BoW)` are in the dataset. We can test given approach"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess(url:str)->None:\n",
        "  \"\"\"Download and remove metadata\"\"\"\n",
        "  dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "  dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "  train_dir = os.path.join(dataset_dir, 'train')\n",
        "  remove_dir = os.path.join(train_dir, 'unsup')\n",
        "  shutil.rmtree(remove_dir) \n",
        "  os.remove(os.path.join(dataset_dir,'imdb.vocab'))\n",
        "  os.remove(os.path.join(dataset_dir,'imdbEr.txt'))\n",
        "  os.remove(os.path.join(dataset_dir,'README'))\n",
        "\n",
        "  remove_feat = glob.glob(f'{dataset_dir}/*/*.feat')\n",
        "  remove_urls = glob.glob(f'{dataset_dir}/*/urls_*.txt')\n",
        "\n",
        "  for filePath in remove_feat + remove_urls:\n",
        "    os.remove(filePath)"
      ],
      "metadata": {
        "id": "1b8-gmiIw8dY"
      },
      "id": "1b8-gmiIw8dY",
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ds(dataset_path:str) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
        "  \"\"\"Load dataset\"\"\"\n",
        "  ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    dataset_path, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    seed=SEED, follow_links=True)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "jitAQU1w0e3q"
      },
      "id": "jitAQU1w0e3q",
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_preprocess(url=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")"
      ],
      "metadata": {
        "id": "AhWO2Zszzk4k"
      },
      "id": "AhWO2Zszzk4k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_ds(dataset_path='aclImdb')"
      ],
      "metadata": {
        "id": "lebb-ljjhzzP"
      },
      "id": "lebb-ljjhzzP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#todo split and join ds\n",
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        ds = ds.shuffle(shuffle_size, seed=SEED)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "VrIT6M2tfK2-"
      },
      "id": "VrIT6M2tfK2-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds, raw_val_ds, raw_test_ds = get_dataset_partitions_tf(ds,50000)"
      ],
      "metadata": {
        "id": "5crWmv2OYj0A"
      },
      "id": "5crWmv2OYj0A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394428cb",
      "metadata": {
        "id": "394428cb"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54cb9d0a",
      "metadata": {
        "id": "54cb9d0a"
      },
      "outputs": [],
      "source": [
        "max_features = 20000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f8e886",
      "metadata": {
        "id": "87f8e886"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241bc67a",
      "metadata": {
        "id": "241bc67a"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a02bb1",
      "metadata": {
        "id": "d5a02bb1"
      },
      "outputs": [],
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be4dc8f",
      "metadata": {
        "id": "8be4dc8f"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0e12bb1",
      "metadata": {
        "id": "d0e12bb1"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45084468",
      "metadata": {
        "id": "45084468"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(max_features + 1, embedding_dim),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.LSTM(100),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1)],\n",
        "    name='lstm_model')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = \"logs/lstm-model\"\n",
        "checkpoint_path = \"models/lstm-model/training__{epoch:02d}__{loss:.6f}/cp.ckpt\"\n",
        "\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=20,monitor='binary_accuracy')\n",
        "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='binary_accuracy', factor=0.5, patience=10, verbose=1, mode='auto',\n",
        "    min_delta=0.0001, cooldown=0, min_lr=0.00001\n",
        ")"
      ],
      "metadata": {
        "id": "p4VC8NMEi8Vf"
      },
      "id": "p4VC8NMEi8Vf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3aeadd7",
      "metadata": {
        "id": "d3aeadd7"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.2),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0),\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf2debb",
      "metadata": {
        "id": "8cf2debb"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=40,\n",
        "    callbacks=[\n",
        "      tensorboard_callback, \n",
        "      cp_callback, \n",
        "      early_stop_callback, \n",
        "      reduce_lr_callback],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1055b3f2",
      "metadata": {
        "id": "1055b3f2"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "8sanjaXImO2Q"
      },
      "id": "8sanjaXImO2Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "60947b28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "60947b28",
        "outputId": "883b2cd2-18a7-44e8-cd38-341f491bf36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:921: RuntimeWarning: divide by zero encountered in log10\n",
            "  numdigits = int(np.log10(self.target)) + 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-c98187cab670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_test_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'd/%d ['\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
          ]
        }
      ],
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "baa6a501",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baa6a501",
        "outputId": "61c0a9ed-4b40-4aed-a821-c11bf46ddfc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5174026 ],\n",
              "       [0.51741004],\n",
              "       [0.5173913 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ],
      "source": [
        "examples = [\n",
        "  \"The movie was great!\",\n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was terrible...\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C85G097hca5I"
      },
      "id": "C85G097hca5I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "CNN_IMDB_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}